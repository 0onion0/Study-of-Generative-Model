{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a779e0f8",
   "metadata": {},
   "source": [
    "# 오토인코더의 모든 것 (1)\n",
    "\n",
    " https://www.youtube.com/watch?v=o_peo6U7IRM\n",
    "\n",
    "본 자료는 위 링크로 이어지는 유튜브 강의를 정리한 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78a9261",
   "metadata": {},
   "source": [
    "## 0. Introduction  \n",
    "네 가지 키워드  \n",
    "\n",
    "오토인코더 학습 시:  \n",
    "- 1. Unsupervised learning \n",
    "- 2. ML density estimation (Loss가 negative ML)  \n",
    "  \n",
    "학습된 오토인코더에서:  \n",
    "- 3. Generative model learning - 디코더\n",
    "- 4. Manifold learning (차원 축소) - 인코더"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8de547",
   "metadata": {},
   "source": [
    "## 1. Revisit Deep Neural Networks\n",
    "  \n",
    "  요약: DNN 학습 과정은 두 가지 관점에서 해석할 수 있다.\n",
    "  - 역전파: CE 역전파시 첫 항에서 활성화함수 미분값이 없어지므로 초기값에 둔감하며, MSE보다 학습이 잘됨.\n",
    "  - Maximum likelihood (확률분포의 파라미터를 찾음.): 네트워크의 출력값이 연속값이면 MSE, 불연속값이면(분류) CE.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d92d54",
   "metadata": {},
   "source": [
    "### (1) Machine learning problem  \n",
    "  \n",
    "  머신러닝 과정:    \n",
    "  1) collecting data  (x,y)\n",
    "  2) Defining functions  - 모델 종류(f_θ(x)), 로스 ftns(L(f,y)) (<-아무거나 쓰면 안됨 by backpropagation)  \n",
    "   - 역전파 조건: \n",
    "      - 훈련 샘플의 전체 loss는 각 샘플의 loss의 합과 같다.\n",
    "      - 로스함수의 구성은 네트워크의 출력만으로 구성된다.\n",
    "  3) Learning/Training  - 최적의 parameters 찾기\n",
    "  4) Predicting/Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204d611a",
   "metadata": {},
   "source": [
    "### (2) Loss function viewpoints I : Back-propagation\n",
    "1. Collecting data\n",
    "2. Defining functions  \n",
    "- f = DNN  \n",
    "- θ(parameters) : w(weight), b(bias)  \n",
    "- L = MSE, CE ...  \n",
    "3. Learning/Training  \n",
    "최적화 방법: Gradient Descent(Iterative method)  \n",
    "- θ 업데이트 방법은?  \n",
    "    - loss 감소하는 방향.\n",
    "- 언제 멈춤? \n",
    "    - loss 변화 없을 때.\n",
    "- Δθ 어케 찾음?    \n",
    "    - Δθ = -η∇L   \n",
    "  (Tayler Expansion. 이때 테일러 급수의 1차 미분항만으로 근사했기 때문에 매우 좁은 영역에서만 감소 방향이 정확하다. 따라서 작은 lr을 사용함.)\n",
    "4. Predicting/Testing\n",
    "- 역전파 과정에서의 Gradiant Vanishing :  \n",
    "  sigmoid 미분값이 0에 가까울수록 학습이 어려움.   \n",
    "  MSE가 더 심함 -> CE가 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110167d1",
   "metadata": {},
   "source": [
    "### (3) Loss function viewpoints II : Maximum likelihood\n",
    "1. Collecting data\n",
    "2. Defining functions  \n",
    "- θ(parameters) =  p(y|f(x))가 최대가 되는 파라미터\n",
    "- L = Gaussian distribution(MSE와 같음), Bernoulli distribution(CE와 같음)\n",
    "3. Learning/Training  \n",
    "4. Predicting/Testing  \n",
    "  \n",
    "\n",
    "네트워크의 출력값이 연속값이면 MSE, 불연속값이면(분류) CE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c22c1f",
   "metadata": {},
   "source": [
    "### (4) Maximum likelihood for autoencoders\n",
    "\n",
    "Maximun likelihood의 결론이 유효한 상태에서,  \n",
    "- AE : p(x|x)  \n",
    "- VAE : p(x)  \n",
    "의 확률 분포를 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3301de92",
   "metadata": {},
   "source": [
    "## 2. Manifold Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f763117",
   "metadata": {},
   "source": [
    "## 3. Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a7ba16",
   "metadata": {},
   "source": [
    "## 4. Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d984cb4",
   "metadata": {},
   "source": [
    "## 5. Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6203ac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
